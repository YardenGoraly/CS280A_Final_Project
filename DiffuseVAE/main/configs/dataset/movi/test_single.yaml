# DDPM config used for DDPM training
ddpm:
  data:   # Most of these have same meaning as in `cifar10/train.yaml`
    root: ???
    name: "movi"
    image_size: 128
    hflip: True
    n_channels: 3
    norm: True
    ddpm_latent_path: ""   # If sharing DDPM latents between diffusevae samples, path to .pt tensor containing latent codes

  model:   # Most of these have same meaning as in `cifar10/train.yaml`
    dim : 128
    attn_resolutions: "16,"
    n_residual: 2
    dim_mults: "1,2,2,2"
    dropout: 0.1
    n_heads: 1
    beta1: 0.0001
    beta2: 0.02
    n_timesteps: 1000

  evaluation:
    chkpt_path: '/home/leohuang/data/ckpt/ddpmv2-movi_uncond_nheads=8_dropout=0.3-epoch=03-loss=0.0000.ckpt'   # DiffuseVAE checkpoint path
    save_path: '/home/leohuang/data/test_single/uncond_recons/'   # Path to write samples to (automatically creates directories if needed)
    z_cond: False   # Whether to condition UNet on vae latent
    z_dim: 512   # Dimensionality of the vae latent
    guidance_weight: 0.0   # Guidance weight during sampling if using Classifier free guidance
    type: 'uncond'   # DiffuseVAE type. One of ['form1', 'form2', 'uncond']. `uncond` is baseline DDPM
    resample_strategy: "spaced"   # Whether to use spaced or truncated sampling. Use 'truncated' if sampling for the entire 1000 steps
    skip_strategy: "uniform"   # Skipping strategy to use if `resample_strategy=spaced`. Can be ['uniform', 'quad'] as in DDIM
    sample_method: "ddpm"   # Sampling backend. Can be ['ddim', 'ddpm']
    sample_from: "target"   # Whether to sampling from the (non)-EMA model. Can be ['source', 'target']
    seed: 0   # Random seed during sampling
    device: "gpu:2"   # Device. Uses TPU/CPU if set to `tpu` or `cpu`. For GPU, use gpu:<comma separated id list>. Ex: gpu:0,1 would run only on gpus 0 and 1
    n_samples: 8
    n_steps: 4   # Number of reverse process steps to use during sampling. Typically [0-100] for DDIM and T=1000 for DDPM
    workers: 4
    batch_size: 8   # Batch size during sampling per gpu
    save_vae: False   # Whether to save VAE samples along with final samples. Useful to visualize the generator-refiner framework in action!
    variance: "fixedsmall"   # DDPM variance to use when using DDPM. Can be ['fixedsmall', 'fixedlarge']
    sample_prefix: "gpu_2"   # Prefix used in naming when saving samples to disk
    temp: 1.0   # Temperature sampling factor in DDPM latents
    save_mode: image   # Whether to save samples as .png or .npy. One of ['image', 'numpy']
  
  interpolation:
    n_steps: 10

# VAE config used for VAE training
vae:
  data:
    root: ???
    name: "cifar10"
    image_size: 32
    n_channels: 3

  model:
    z_dim: 512
    enc_block_config : "32x7,32d2,32t16,16x4,16d2,16t8,8x4,8d2,8t4,4x3,4d4,4t1,1x3"
    enc_channel_config: "32:64,16:128,8:256,4:256,1:512"
    dec_block_config: "1x1,1u4,1t4,4x2,4u2,4t8,8x3,8u2,8t16,16x7,16u2,16t32,32x15"
    dec_channel_config: "32:64,16:128,8:256,4:256,1:512"

  evaluation:
    chkpt_path: ???
    save_path: ???
    expde_model_path: ""
    seed: 0
    device: "gpu:0"
    workers: 2
    batch_size: 8
    n_samples: 50000
    sample_prefix: ""
    save_mode: image
